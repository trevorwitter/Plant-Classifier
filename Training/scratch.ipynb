{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca009c61-a508-41b4-a760-abc3001c13b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.io import read_image\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd54c92a-10aa-4968-81fe-8377795d1651",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from torchvision.io import read_image\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c789d13-4d2c-40bc-9857-ad05dee4078c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#class LabelTransform():\n",
    "#    def __init__(self):\n",
    "#        df = pd.read_csv('../Training/data/annotations_train_file.csv')\n",
    "#        self.num_classes = len(df['label'].unique())\n",
    "#    \n",
    "#    def transform(self, labels):\n",
    "#        labels_tensor = torch.tensor(labels)\n",
    "#        one_hot = F.one_hot(labels_tensor, num_classes=self.num_classes)\n",
    "#        return one_hot.type(torch.float32)\n",
    "\n",
    "def label_transform(labels):\n",
    "    labels_tensor = torch.tensor(labels)\n",
    "    one_hot = F.one_hot(labels_tensor, num_classes=4271)\n",
    "    return one_hot.type(torch.float32)\n",
    "\n",
    "class PlantImageDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"For working with plant images from the inaturalist dataset\"\"\"\n",
    "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        image = read_image(img_path)\n",
    "        #label = self.img_labels.iloc[idx, 1]\n",
    "        label = self.img_labels.iloc[idx, 1:]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label\n",
    "    \n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        #self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc1 = nn.Linear(44944, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 4271)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32f9530-db63-4cd5-a4ec-b5e127e13388",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.Resize(256,antialias=True),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ConvertImageDtype(torch.float32),\n",
    "    #transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "#label_transform = LabelTransform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48af0023-dccd-4ef1-8fdc-8eb38692023a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#label_transform.num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a06af3-8ab2-4f25-a446-5891d3a17a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = PlantImageDataset(annotations_file=\"data/annotations_train_file.csv\",\n",
    "                         img_dir=\"data/2021_train_mini/\",\n",
    "                         transform=transform,\n",
    "                         target_transform=label_transform\n",
    "                              )\n",
    "\n",
    "val_data = PlantImageDataset(annotations_file=\"../Training/data/annotations_val_file.csv\",\n",
    "                         img_dir=\"../Training/data/2021_train_mini/\",\n",
    "                         transform=transform,\n",
    "                         target_transform=label_transform\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1896371a-8650-4e7b-88f7-7088306b46a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(train_data, batch_size=4, shuffle=False, num_workers=1)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_data, batch_size=4, shuffle=False, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e92f3cc-cccc-4406-99b7-dc1435f8f8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = next(iter(train_dataloader))\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812fcec6-f296-4297-8084-8b940b99f609",
   "metadata": {},
   "outputs": [],
   "source": [
    "image, label = next(iter(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868e60b6-570a-4899-a474-adefe3d62563",
   "metadata": {},
   "outputs": [],
   "source": [
    "label.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc66d812-823c-48a1-a012-7a65dc20f49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c65427c-711e-4519-9893-1839258ad498",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
